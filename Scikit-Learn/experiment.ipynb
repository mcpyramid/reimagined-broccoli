{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis spacy gensim\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in terminal or command prompt\n",
    "#!python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "from scipy import sparse\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time as timer\n",
    "import time\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === TERM FREQUENCY FUNCTIONS ===\n",
    "lambda_step = 0.01\n",
    "n_jobs = -1\n",
    "R = 100\n",
    "sort_topics = True\n",
    "\n",
    "def _find_relevance(log_ttd, log_lift, R, lambda_):\n",
    "    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n",
    "    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)\n",
    "\n",
    "\n",
    "def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):\n",
    "    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])\n",
    "\n",
    "def _chunks(l, n):\n",
    "    \"\"\" Yield successive n-sized chunks from l.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def _job_chunks(l, n_jobs):\n",
    "    n_chunks = n_jobs\n",
    "    if n_jobs < 0:\n",
    "        # so, have n chunks if we are using all n cores/cpus\n",
    "        n_chunks = cpu_count() + 1 - n_jobs\n",
    "\n",
    "    return _chunks(l, n_chunks)\n",
    "\n",
    "def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,\n",
    "                vocab, lambda_step, R, n_jobs):\n",
    "    # marginal distribution over terms (width of blue bars)\n",
    "    term_proportion = term_frequency / term_frequency.sum()\n",
    "\n",
    "    # compute the distinctiveness and saliency of the terms:\n",
    "    # this determines the R terms that are displayed when no topic is selected\n",
    "    topic_given_term = topic_term_dists / topic_term_dists.sum()\n",
    "    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
    "    distinctiveness = kernel.sum()\n",
    "    saliency = term_proportion * distinctiveness\n",
    "    # Order the terms for the \"default\" view by decreasing saliency:\n",
    "    default_term_info = pd.DataFrame({\n",
    "        'saliency': saliency,\n",
    "        'Term': vocab,\n",
    "        'Freq': term_frequency,\n",
    "        'Total': term_frequency,\n",
    "        'Category': 'Default'})\n",
    "    default_term_info = default_term_info.sort_values(\n",
    "        by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
    "    # Rounding Freq and Total to integer values to match LDAvis code:\n",
    "    default_term_info['Freq'] = np.floor(default_term_info['Freq'])\n",
    "    default_term_info['Total'] = np.floor(default_term_info['Total'])\n",
    "    ranks = np.arange(R, 0, -1)\n",
    "    default_term_info['logprob'] = default_term_info['loglift'] = ranks\n",
    "\n",
    "    # compute relevance and top terms for each topic\n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)\n",
    "\n",
    "    def topic_top_term_df(tup):\n",
    "        new_topic_id, (original_topic_id, topic_terms) = tup\n",
    "        term_ix = topic_terms.unique()\n",
    "        return pd.DataFrame({'Term': vocab[term_ix],\n",
    "                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],\n",
    "                             'Total': term_frequency[term_ix],\n",
    "                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),\n",
    "                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),\n",
    "                             'Category': 'Topic%d' % new_topic_id})\n",
    "\n",
    "    top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n",
    "                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n",
    "                          for ls in _job_chunks(lambda_seq, n_jobs)))\n",
    "    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))\n",
    "    return pd.concat([default_term_info] + list(topic_dfs), sort=True)\n",
    "\n",
    "\n",
    "def _token_table(topic_info, term_topic_freq, vocab, term_frequency):\n",
    "    # last, to compute the areas of the circles when a term is highlighted\n",
    "    # we must gather all unique terms that could show up (for every combination\n",
    "    # of topic and value of lambda) and compute its distribution over topics.\n",
    "\n",
    "    # term-topic frequency table of unique terms across all topics and all values of lambda\n",
    "    term_ix = topic_info.index.unique()\n",
    "    term_ix = np.sort(term_ix)\n",
    "\n",
    "    top_topic_terms_freq = term_topic_freq[term_ix]\n",
    "    # use the new ordering for the topics\n",
    "    K = len(term_topic_freq)\n",
    "    top_topic_terms_freq.index = range(1, K + 1)\n",
    "    top_topic_terms_freq.index.name = 'Topic'\n",
    "\n",
    "    # we filter to Freq >= 0.5 to avoid sending too much data to the browser\n",
    "    token_table = pd.DataFrame({'Freq': top_topic_terms_freq.unstack()})\\\n",
    "        .reset_index().set_index('term').query('Freq >= 0.5')\n",
    "\n",
    "    token_table['Freq'] = token_table['Freq'].round()\n",
    "    token_table['Term'] = vocab[token_table.index.values].values\n",
    "    # Normalize token frequencies:\n",
    "    token_table['Freq'] = token_table.Freq / term_frequency[token_table.index]\n",
    "    return token_table.sort_values(by=['Term', 'Topic'])\n",
    "\n",
    "def _get_doc_lengths(dtm):\n",
    "    return dtm.sum(axis=1).getA1()\n",
    "\n",
    "\n",
    "def _get_term_freqs(dtm):\n",
    "    return dtm.sum(axis=0).getA1()\n",
    "\n",
    "\n",
    "def _get_vocab(vectorizer):\n",
    "    return vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "def _row_norm(dists):\n",
    "    # row normalization function required\n",
    "    # for doc_topic_dists and topic_term_dists\n",
    "    return dists / dists.sum(axis=1)[:, None]\n",
    "\n",
    "\n",
    "def _get_doc_topic_dists(lda_model, dtm):\n",
    "    return _row_norm(lda_model.transform(dtm))\n",
    "\n",
    "\n",
    "def _get_topic_term_dists(lda_model):\n",
    "    return _row_norm(lda_model.components_)\n",
    "\n",
    "def _df_with_names(data, index_name, columns_name):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        # we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def _series_with_name(data, name):\n",
    "    if type(data) == pd.Series:\n",
    "        data.name = name\n",
    "        # ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)\n",
    "    \n",
    "\n",
    "def _get_doc_topic_dists(lda_model, dtm):\n",
    "    return _row_norm(lda_model.transform(dtm))\n",
    "\n",
    "\n",
    "def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "    ttds = topic_term_dists.shape\n",
    "    dtds = doc_topic_dists.shape\n",
    "    errors = []\n",
    "\n",
    "    def err(msg):\n",
    "        errors.append(msg)\n",
    "\n",
    "    if dtds[1] != ttds[0]:\n",
    "        err_msg = ('Number of rows of topic_term_dists does not match number of columns of '\n",
    "                   'doc_topic_dists; both should be equal to the number of topics in the model.')\n",
    "        err(err_msg)\n",
    "\n",
    "    if len(doc_lengths) != dtds[0]:\n",
    "        err_msg = ('Length of doc_lengths not equal to the number of rows in doc_topic_dists;'\n",
    "                   'both should be equal to the number of documents in the data.')\n",
    "        err(err_msg)\n",
    "\n",
    "    W = len(vocab)\n",
    "    if ttds[1] != W:\n",
    "        err_msg = ('Number of terms in vocabulary does not match the number of columns of '\n",
    "                   'topic_term_dists (where each row of topic_term_dists is a probability '\n",
    "                   'distribution of terms for a given topic)')\n",
    "        err(err_msg)\n",
    "    if len(term_frequency) != W:\n",
    "        err_msg = ('Length of term_frequency not equal to the number of terms in the '\n",
    "                   'number of terms in the vocabulary (len of vocab)')\n",
    "        err(err_msg)\n",
    "\n",
    "    if __num_dist_rows__(topic_term_dists) != ttds[0]:\n",
    "        err('Not all rows (distributions) in topic_term_dists sum to 1.')\n",
    "\n",
    "    if __num_dist_rows__(doc_topic_dists) != dtds[0]:\n",
    "        err('Not all rows (distributions) in doc_topic_dists sum to 1.')\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        return errors\n",
    "\n",
    "\n",
    "def _input_validate(*args):\n",
    "    res = _input_check(*args)\n",
    "    if res:\n",
    "        raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n",
    "        \n",
    "\n",
    "def __num_dist_rows__(array, ndigits=2):\n",
    "    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())\n",
    "\n",
    "\n",
    "class ValidationError(ValueError):\n",
    "    pass\n",
    "\n",
    "def build_term_frequency(lda_model, sparse_matrix, vectorizer):\n",
    "    vocab = _get_vocab(vectorizer)\n",
    "    doc_lengths = _get_doc_lengths(sparse_matrix)\n",
    "    term_freqs = _get_term_freqs(sparse_matrix)\n",
    "    topic_term_dists = _get_topic_term_dists(lda_model)\n",
    "    doc_topic_dists = _get_doc_topic_dists(lda_model, sparse_matrix)\n",
    "\n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_freqs, 'term_frequency')\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "    \n",
    "    t0 = timer()\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    # topic_freq       = np.dot(doc_topic_dists.T, doc_lengths)\n",
    "    if (sort_topics):\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "\n",
    "    topic_order = topic_proportion.index\n",
    "    # reorder all data based on new ordering of topics\n",
    "    topic_freq = topic_freq[topic_order]\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "    doc_topic_dists = doc_topic_dists[topic_order]\n",
    "\n",
    "    # token counts for each term-topic combination (widths of red bars)\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    # Quick fix for red bar width bug.  We calculate the\n",
    "    # term frequencies internally, using the topic term distributions and the\n",
    "    # topic frequencies, rather than using the user-supplied term frequencies.\n",
    "    # For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "\n",
    "    topic_info = _topic_info(topic_term_dists, topic_proportion,\n",
    "                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n",
    "    token_table = _token_table(topic_info, term_topic_freq, vocab, term_frequency)\n",
    "\n",
    "    print(\"done in %0.3fs.\" % (timer() - t0))\n",
    "    return { 'topic_info': topic_info, 'token_table': token_table }\n",
    "\n",
    "def preprocess(data):\n",
    "    print(\"Processing {} of data\".format(len(data)))\n",
    "    # Remove Emails\n",
    "    data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            #yield(gensim.utils.tokenize(str(sentence), deacc=True))\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    #print (\"=== DATA WORDS ===\")\n",
    "    #print(data_words)\n",
    "    \n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "        return texts_out\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # Run in terminal: python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser','ner'])\n",
    "\n",
    "    # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "    data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    #print (\"=== LEMMATIZED ===\")\n",
    "    #print(data_lemmatized)\n",
    "    vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=2,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=False,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{5,}',  # num chars > 3\n",
    "                             max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "    \n",
    "    data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "    return vectorizer, data_vectorized\n",
    "\n",
    "def get_words(vectorizer, lda_model):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        topic_keyword_location = (-topic_weights).argsort()[:20]\n",
    "        topic_keywords.append(keywords.take(topic_keyword_location))\n",
    "    return topic_keywords\n",
    "\n",
    "def build_LDA(data_vectorized, n_components=10):\n",
    "    # Build LDA Model\n",
    "    print(\"Data Vectorized Length: \", data_vectorized.shape)\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_components,               # Number of topics\n",
    "                                          max_iter=10,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=128,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "                                                                                        \n",
    "    print(lda_model)  # Model attributes\n",
    "    return lda_model, lda_output\n",
    "                                                                                                                            \n",
    "def build_KMeans(lda_output, n_clusters=10):\n",
    "    # Construct the k-means clusters\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=100)\n",
    "    clusters = kmeans.fit_predict(lda_output)\n",
    "\n",
    "    # Build the Singular Value Decomposition(SVD) model\n",
    "    svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "    lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "    # X and Y axes of the plot using SVD decomposition\n",
    " #   x = lda_output_svd[:, 0]\n",
    " #   y = lda_output_svd[:, 1]\n",
    "\n",
    "    # Weights for the 15 columns of lda_output, for each component\n",
    "  #  print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
    "\n",
    "    # Percentage of total information in 'lda_output' explained by the two components\n",
    "   # print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))\n",
    "    return kmeans, clusters\n",
    "#    return kmeans, clusters, x, y\n",
    "\n",
    "def build_pyLDAvis(lda_model, sparse_matrix, vectorizer):\n",
    "    panel = pyLDAvis.sklearn.prepare(lda_model, sparse_matrix, vectorizer, mds='tsne')\n",
    "    return panel\n",
    "\n",
    "def get_sector_names(sector_panels):\n",
    "    sector_names = []\n",
    "    # for i in range(len(sector_panels)):\n",
    "        #print (\"Getting panel \", i)\n",
    "        #sector_panel_token_table = sector_panels[i]['token_table']\n",
    "    sector_panel_token_table = sector_panels['token_table'] #TODO delete me because I am just a single, not a collection\n",
    "    mostFrequentTerm = sector_panel_token_table.loc[[sector_panel_token_table.Freq.idxmax()]]\n",
    "    print(mostFrequentTerm['Term'].max())\n",
    "    sector_names.append(mostFrequentTerm['Term'].max())\n",
    "    sector_names_df = pd.DataFrame(sector_names,columns=['Sector Names'])\n",
    "    return sector_names_df\n",
    "\n",
    "def get_industry_names(industry_panels, sectorIndex):\n",
    "    industry_names = []\n",
    "    for i in range(len(industry_panels)):\n",
    "        print (\"Getting panel \", i)\n",
    "        industry_panel_token_table = industry_panels[i]['token_table']\n",
    "        mostFrequentTerm = industry_panel_token_table.loc[[industry_panel_token_table.Freq.idxmax()]]\n",
    "        print(mostFrequentTerm['Term'].max())\n",
    "        industry_names.append([mostFrequentTerm['Term'].max(), i, sectorIndex])\n",
    "    industry_names_df = pd.DataFrame(industry_names,columns=['Industry Names', 'Industry Index', 'Sector Index'])\n",
    "    return industry_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('s3://mctestraaa-pipeline-data/data/train/test.csv')\n",
    "t0 = timer()\n",
    "# Root process\n",
    "countVectorizer, data_vectorized = preprocess(df.Article)\n",
    "word_vectorized_df = pd.DataFrame(data_vectorized.toarray())\n",
    "word_vectorized_df.head()\n",
    "print(\"done in %0.3fs.\" % (timer() - t0))\n",
    "\n",
    "t0 = timer()\n",
    "lda_model, lda_output = build_LDA(word_vectorized_df)\n",
    "kmeans, clusters = build_KMeans(lda_output)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (timer() - t0))\n",
    "clusters_df = pd.DataFrame(clusters, columns=['Cluster Index'])\n",
    "sectors_df = pd.concat([df, clusters_df], axis=1)\n",
    "sectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it's grouped let's iterate each cluster and scope LDA to just those words\n",
    "t0 = timer()\n",
    "sector_panels = []\n",
    "industry_panels = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "#for i in range(2):\n",
    "    industry_panels = []\n",
    "    \n",
    "    print (\"Processing Sector Cluster \", i)\n",
    "    cluster_n = sectors_df[sectors_df['Cluster Index'] == i]\n",
    "    print (\"Data Length: \", len(cluster_n))\n",
    "    cluster_n_word_vec_df = word_vectorized_df.iloc[cluster_n.index.values]\n",
    "    print(cluster_n.head())\n",
    "    \n",
    "    print (\"Word Data Length: \", len(cluster_n_word_vec_df))\n",
    "    \n",
    "    print (\"=== Building LDA for Sector ===\")\n",
    "    cluster_n_lda_model, cluster_n_lda_output = build_LDA(cluster_n_word_vec_df, 10)\n",
    "    sparse_matrix = sparse.csr_matrix(cluster_n_word_vec_df)\n",
    "    \n",
    "    print (\"=== Building K Means for Industries ===\")\n",
    "    industry_kmeans, industry_clusters = build_KMeans(cluster_n_lda_output, cluster_n_lda_output.shape[0])\n",
    "    \n",
    "    # Zip the given sector index (key) with the industry index and set the industry index back to the original frame\n",
    "    combined = zip(cluster_n.index.values, industry_clusters)\n",
    "    for sectorIndex, industryIndex in combined:\n",
    "        sectors_df.at[sectorIndex, 'Industry Index'] = industryIndex\n",
    "    \n",
    "    for industry_index in range(industry_kmeans.n_clusters):\n",
    "        print (\"Processing Industry Cluster \", industry_index)\n",
    "        # Get the original key index given an industry index\n",
    "        originalKeysFromDataframe = sectors_df[sectors_df['Industry Index'] == industry_index].index.values\n",
    "        \n",
    "        # Now do the same and get the vectorized words given the keys\n",
    "        industry_n_word_vec_df = word_vectorized_df.iloc[originalKeysFromDataframe]\n",
    "        \n",
    "        industry_n_lda_model, industry_n_lda_output = build_LDA(industry_n_word_vec_df, 10)\n",
    "        \n",
    "        industry_sparse_matrix = sparse.csr_matrix(industry_n_word_vec_df)\n",
    "        \n",
    "        # Now get the best topic and keyword frequency to get industsy name\n",
    "        print (\"=== Building Term Frequency for Industry ===\")\n",
    "        industry_panel = build_term_frequency(industry_n_lda_model, industry_sparse_matrix, countVectorizer)\n",
    "        industry_panels.append(industry_panel)\n",
    "    \n",
    "    industry_names_df = get_industry_names(industry_panels, i)\n",
    "    \n",
    "    # Set the industry names back to the original dataset\n",
    "    for index, row in industry_names_df.iterrows():\n",
    "        indexKeys = sectors_df[(sectors_df['Cluster Index'] == row['Sector Index']) \n",
    "                         & (sectors_df['Industry Index'] == row['Industry Index'])].index.values\n",
    "        for key in indexKeys:\n",
    "            sectors_df.at[key, 'Industry Names'] = row['Industry Names']\n",
    "    \n",
    "    \n",
    "    print (\"===  LDA Components ===\", cluster_n_lda_model.components_)\n",
    "    print (\"=== Building Term Frequency for Sector ===\")\n",
    "    sector_panel = build_term_frequency(cluster_n_lda_model, sparse_matrix, countVectorizer)\n",
    "    sector_names_df = get_sector_names(sector_panel)\n",
    "    # Set the sector name back to the original dataset TODO: Maybe create a function\n",
    "    \n",
    "    for index, row in sector_names_df.iterrows():\n",
    "        indexKeys = sectors_df[(sectors_df['Cluster Index'] == index)].index.values\n",
    "        for key in indexKeys:\n",
    "            sectors_df.at[key, 'Sector Names'] = row['Sector Names']\n",
    "print(\"done in %0.3fs.\" % (timer() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors_df.to_csv('output-with-industry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
